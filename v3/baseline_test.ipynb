{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59353c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_case.baseline import * \n",
    "from tests.eval import *\n",
    "\n",
    "np.random.seed(32042)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "N = 10\n",
    "payoff_i = np.random.uniform(-10, 10, (N, N))\n",
    "payoff_j = np.random.uniform(-10, 10, (N, N))\n",
    "\n",
    "# Initialize environment\n",
    "N_ACTIONS = payoff_i.shape[0]\n",
    "N_AGENTS = 500\n",
    "env = BaselineEnvironment(N_AGENTS, payoff_i, payoff_j, total_games = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8866f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate average rewards for all action pairs\n",
    "average_rewards = (payoff_i + payoff_j) / 2\n",
    "\n",
    "# Flatten the matrix into a list of all possible rewards\n",
    "all_rewards = average_rewards.ravel()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_rewards, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Average Rewards for All Action Pairs')\n",
    "plt.xlabel('Average Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate average rewards for all action pairs\n",
    "max_rewards = np.max([payoff_i, payoff_j], axis = 0)\n",
    "\n",
    "# Flatten the matrix into a list of all possible rewards\n",
    "all_rewards = max_rewards.ravel()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_rewards, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Average Rewards for All Action Pairs')\n",
    "plt.xlabel('Average Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af11f7",
   "metadata": {},
   "source": [
    "# Actual Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3281f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import *\n",
    "from models.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the network here\n",
    "parameters = ParameterSettings(\n",
    "    n_agents = N_AGENTS,\n",
    "    d_action = N_ACTIONS, \n",
    "    d_obs = env.obs_size, \n",
    "    d_traits = 1,\n",
    "    d_het_latent = 4\n",
    ")\n",
    "parameters.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = PPOModel(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "equilibriua = find_pure_equilibria(payoff_i, payoff_j)\n",
    "\n",
    "for eq in equilibriua:\n",
    "    x, y = eq \n",
    "    a = (y[0] + y[1]) / 2\n",
    "\n",
    "    print(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training loop\n",
    "training_parameters = TrainingParameters(\n",
    "    outer_loops = 1000,\n",
    "    \n",
    "    actor_learning_rate= 1e-4,\n",
    "    critic_learning_rate = 1e-4,\n",
    "    hypernet_learning_rate = 1e-4,\n",
    "\n",
    "    hypernet_jsd_threshold = 2.0,\n",
    "    hypernet_samples = 3000,\n",
    "    hypernet_jsd_weight = 0.01,\n",
    "    hypernet_entropy_weight = 0.8, \n",
    "    hypernet_diversity_weight= 0.8,\n",
    "\n",
    "    sampled_agents_proportion = 1.0,\n",
    "    experience_sampling_steps = 1,\n",
    "    experience_buffer_size = 10,\n",
    "\n",
    "    entropy_coeff = 0.2,\n",
    "\n",
    "    epsilon_period = 0,\n",
    "\n",
    "    eval_temp = -1.0,\n",
    "    noise_scale = 2.0,\n",
    "    \n",
    "    should_train_hypernet = True,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "train_model(model, env, training_parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c579f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83f73be",
   "metadata": {},
   "source": [
    "# Heterogeneous Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ef618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7afc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_case.baseline import * \n",
    "from tests.eval import *\n",
    "from models.model import *\n",
    "from models.trainer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09398cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "N_AGENTS = 500\n",
    "N_TYPES = 10\n",
    "N_ACTIONS = 10\n",
    "type_payoffs = np.random.uniform(-10, 10, (N_TYPES, N_TYPES, 2, N_ACTIONS, N_ACTIONS))\n",
    "env = BaselineHeterogeneous(N_AGENTS, N_TYPES, type_payoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the network here\n",
    "parameters = ParameterSettings(\n",
    "    n_agents = N_AGENTS,\n",
    "    d_action = N_ACTIONS, \n",
    "    d_obs = env.obs_size, \n",
    "    d_traits = N_TYPES,\n",
    "    d_beliefs = 1\n",
    "    \n",
    ")\n",
    "parameters.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SACModel(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09097f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training loop\n",
    "training_parameters = TrainingParameters(\n",
    "    outer_loops = 1_000,\n",
    "    \n",
    "    actor_learning_rate= 1e-4,\n",
    "    critic_learning_rate = 1e-4,\n",
    "    hypernet_learning_rate = 1e-4,\n",
    "\n",
    "\n",
    "    hypernet_samples_per_batch =  0.2,\n",
    "    hypernet_jsd_threshold = 1.0,\n",
    "\n",
    "    sampled_agents_proportion = 1.0,\n",
    "    experience_sampling_steps = 5,\n",
    "    experience_buffer_size = 25,\n",
    "\n",
    "    entropy_coeff = 0.2,\n",
    "\n",
    "    epsilon_period = 300,\n",
    "    hypernet_entropy_weight = 0.8, \n",
    "    hypernet_diversity_weight= 0.05,\n",
    "    \n",
    "    should_train_hypernet = True,\n",
    "    verbose = False\n",
    ")\n",
    "train_model(model, env, training_parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
