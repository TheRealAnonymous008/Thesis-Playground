{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0764cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59353c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_case.baseline import * \n",
    "from tests.eval import *\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "N = 10\n",
    "payoff_i = np.random.uniform(-10, 10, (N, N))\n",
    "payoff_j = np.random.uniform(-10, 10, (N, N))\n",
    "\n",
    "# Initialize environment\n",
    "N_ACTIONS = payoff_i.shape[0]\n",
    "N_AGENTS = 1000\n",
    "env = BaselineEnvironment(N_AGENTS, payoff_i, payoff_j, total_games = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af11f7",
   "metadata": {},
   "source": [
    "# Actual Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3281f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import *\n",
    "from models.trainer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4f52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the network here\n",
    "parameters = ParameterSettings(\n",
    "    n_agents = N_AGENTS,\n",
    "    d_action = N_ACTIONS, \n",
    "    d_obs = env.obs_size, \n",
    "    d_traits = 1,\n",
    "    d_beliefs = 1\n",
    ")\n",
    "parameters.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Model(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109d0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 10) 9.42934343544164\n"
     ]
    }
   ],
   "source": [
    "equilibriua = find_pure_equilibria(payoff_i, payoff_j)\n",
    "\n",
    "for eq in equilibriua:\n",
    "    x, y = eq \n",
    "    a = (y[0] + y[1]) / 2\n",
    "\n",
    "    print(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db5dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Return: 0.0656039389081976\n",
      "    Total returns: 0.656039389081976\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[103  69  83  45  43  65  40  49  66  38  58  41  39  43  33  49  35  37\n",
      "  31  33]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:07<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.009680504910647869\n",
      "    Average Entropy Loss: 36.09934616088867\n",
      "    Average JSD Loss: 0.001468414906412363\n",
      "    \n",
      "\n",
      "    Average Return: 0.11158655686338168\n",
      "    Total returns: 1.1158655686338168\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[132  91  74  79  67  54  28  48  43  44  38  35  52  37  33  24  28  37\n",
      "  19  37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [00:44<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.03668736208463088\n",
      "    Average Value Loss: 24.688346424102782\n",
      "    Average Entropy Loss: 11.961390228271485 \n",
      "    \n",
      "\n",
      "    Average Return: 0.029856960693596103\n",
      "    Total returns: 0.298569606935961\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[144  88  51  67  43  53  38  58  47  38  49  36  30  38  46  32  36  36\n",
      "  33  37]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:18<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.00835129152983427\n",
      "    Average Entropy Loss: 36.1063346862793\n",
      "    Average JSD Loss: 0.001276160473935306\n",
      "    \n",
      "\n",
      "    Average Return: -0.023058099561075594\n",
      "    Total returns: -0.23058099561075596\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[143  89  77  69  68  69  36  40  42  39  42  32  48  43  25  28  29  25\n",
      "  24  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [01:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.0331169086007867\n",
      "    Average Value Loss: 24.65362641811371\n",
      "    Average Entropy Loss: 11.96431843996048 \n",
      "    \n",
      "\n",
      "    Average Return: -0.03998883326398603\n",
      "    Total returns: -0.3998883326398603\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[163 106  87  63  62  44  38  54  47  40  32  34  50  27  23  27  34  24\n",
      "  23  22]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:19<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.006959795951843262\n",
      "    Average Entropy Loss: 36.106109619140625\n",
      "    Average JSD Loss: 0.0011318159522488713\n",
      "    \n",
      "\n",
      "    Average Return: 0.07991809645855877\n",
      "    Total returns: 0.7991809645855876\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[164  98  83  64  62  54  34  41  43  33  55  42  42  28  28  31  27  22\n",
      "  23  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.030431812509777955\n",
      "    Average Value Loss: 24.692739057540894\n",
      "    Average Entropy Loss: 11.966240315437316 \n",
      "    \n",
      "\n",
      "    Average Return: 0.047158621738396\n",
      "    Total returns: 0.47158621738396\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[152  97  77  72  56  47  31  54  43  39  46  43  41  36  40  34  32  22\n",
      "  20  18]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:19<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.007150881923735142\n",
      "    Average Entropy Loss: 36.103145599365234\n",
      "    Average JSD Loss: 0.0009984005009755492\n",
      "    \n",
      "\n",
      "    Average Return: 0.07892089151193413\n",
      "    Total returns: 0.7892089151193413\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[125  77  75  66  60  53  23  61  52  59  36  34  60  26  47  38  31  22\n",
      "  29  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.027382379941846013\n",
      "    Average Value Loss: 24.656230545043947\n",
      "    Average Entropy Loss: 11.967782049179077 \n",
      "    \n",
      "\n",
      "    Average Return: -0.037502076922429683\n",
      "    Total returns: -0.37502076922429683\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[137  86  78  70  59  51  37  62  41  42  34  45  58  30  28  41  27  34\n",
      "  25  15]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:19<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.006244340445846319\n",
      "    Average Entropy Loss: 36.1143798828125\n",
      "    Average JSD Loss: 0.0009325787541456521\n",
      "    \n",
      "\n",
      "    Average Return: 0.2712102925515206\n",
      "    Total returns: 2.712102925515206\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[137  84  93  56  51  60  29  80  46  44  43  37  44  33  35  31  27  36\n",
      "  22  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [01:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.027071181191713548\n",
      "    Average Value Loss: 24.62744049549103\n",
      "    Average Entropy Loss: 11.969071815013885 \n",
      "    \n",
      "\n",
      "    Average Return: 0.15998985423989948\n",
      "    Total returns: 1.5998985423989946\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[193 100  77  71  71  57  34  40  36  42  43  27  42  23  27  32  22  23\n",
      "  24  16]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:13<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.006097221747040749\n",
      "    Average Entropy Loss: 36.11821365356445\n",
      "    Average JSD Loss: 0.000888121488969773\n",
      "    \n",
      "\n",
      "    Average Return: 0.00563343050589254\n",
      "    Total returns: 0.056334305058925394\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[188  93  72  68  71  36  28  46  30  51  49  32  38  34  39  47  25  20\n",
      "  19  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [00:44<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.02111187434196836\n",
      "    Average Value Loss: 24.581277174949648\n",
      "    Average Entropy Loss: 11.97009890794754 \n",
      "    \n",
      "\n",
      "    Average Return: 0.07863347442424173\n",
      "    Total returns: 0.7863347442424173\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[150  84  76  65  82  46  28  51  59  39  55  27  50  45  38  30  23  17\n",
      "  19  16]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:20<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.004827076569199562\n",
      "    Average Entropy Loss: 36.12289047241211\n",
      "    Average JSD Loss: 0.0008067660965025425\n",
      "    \n",
      "\n",
      "    Average Return: 0.1633977935313673\n",
      "    Total returns: 1.633977935313673\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[199 103  90  82  89  72  29  37  37  45  41  32  28  25  17  26  13  14\n",
      "  13   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.020643400607514194\n",
      "    Average Value Loss: 24.590921120643614\n",
      "    Average Entropy Loss: 11.971037621498107 \n",
      "    \n",
      "\n",
      "    Average Return: 0.030386613309970223\n",
      "    Total returns: 0.30386613309970223\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[181 102  96  64  86  43  27  52  29  30  45  34  28  39  38  35  18  30\n",
      "  16   7]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss 0.005737268831580877\n",
      "    Average Entropy Loss: 36.11208724975586\n",
      "    Average JSD Loss: 0.000741052208468318\n",
      "    \n",
      "\n",
      "    Average Return: 0.022475401370138547\n",
      "    Total returns: 0.22475401370138548\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[167 112  86  76  69  68  24  57  31  36  42  29  40  34  25  36  16  21\n",
      "  18  13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor Training: 100%|██████████| 100/100 [00:37<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Average Policy Loss: 0.019926054645766272\n",
      "    Average Value Loss: 24.528240098953248\n",
      "    Average Entropy Loss: 11.971771671772004 \n",
      "    \n",
      "\n",
      "    Average Return: 0.038529306033813084\n",
      "    Total returns: 0.3852930603381308\n",
      "    \n",
      "Action Distribution\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[171  94  73  87  62  61  27  49  39  39  55  30  46  37  22  32  22  22\n",
      "  19  13]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hypernet Loop:  43%|████▎     | 13/30 [00:02<00:03,  4.62it/s]"
     ]
    }
   ],
   "source": [
    "# Setup the training loop\n",
    "training_parameters = TrainingParameters(\n",
    "    outer_loops = 200,\n",
    "    hypernet_training_loops= 5, \n",
    "    actor_training_loops = 100,\n",
    "    experience_buffer_size = 3,\n",
    "\n",
    "    actor_learning_rate= 2.5e-4,\n",
    "    critic_learning_rate = 2.5e-4,\n",
    "    hypernet_learning_rate = 2.5e-4,\n",
    ")\n",
    "\n",
    "train_model(model, env, training_parameters)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
