{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from use_case.tests import * \n",
    "from models.eval import *\n",
    "from models.model import *\n",
    "from models.trainer import *\n",
    "from auto_tests.hypernet_test import * \n",
    "from auto_tests.abm_test import *\n",
    "from models.trainer import load_model\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa6d62",
   "metadata": {},
   "source": [
    "# Hypernet Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd89d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, title_prefix, model_name, result_type=\"Returns\"):\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(30, 12))\n",
    "    fig.suptitle(f\"{result_type} Distributions - {model_name}\", fontsize=24)\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axs[i // 5, i % 5]\n",
    "        ax.hist(data[i], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_title(f'Type {i}', fontsize=16)\n",
    "        ax.set_xlabel(result_type, fontsize=14)\n",
    "        ax.set_ylabel('Frequency', fontsize=14)\n",
    "        ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{model_name}_{result_type.lower()}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def compute_action_entropy(histogram_data, num_actions):\n",
    "    action_counts = np.bincount(histogram_data, minlength=num_actions)\n",
    "    total_actions = np.sum(action_counts)\n",
    "    if total_actions == 0:\n",
    "        return 0.0\n",
    "    probabilities = action_counts / total_actions\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    return entropy\n",
    "\n",
    "env = initialize_baseline_hetero()\n",
    "parameters = ParameterSettings(\n",
    "    n_agents=env.n_agents,\n",
    "    d_action=env.n_actions,\n",
    "    d_obs=env.obs_size,\n",
    "    d_traits=env.d_traits,\n",
    "    d_het_latent=4,\n",
    "    d_beliefs=env.d_beliefs,\n",
    "    d_relation=env.d_relation,\n",
    "    d_message=4,\n",
    "    d_comm_state=env.d_comm_state,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "MODELS_DIR = \"foo\\\\h models\"\n",
    "OUTPUT_DIR = \"output_data\\\\heterogeneous\"\n",
    "\n",
    "run_idx = 1\n",
    "for model_file in os.listdir(MODELS_DIR):\n",
    "    if model_file.endswith(\".pt\"):\n",
    "        file_path = os.path.join(MODELS_DIR, model_file)\n",
    "\n",
    "        try:\n",
    "            model = PPOModel(parameters)\n",
    "            model = load_model(model, file_path)\n",
    "            model.eval()\n",
    "\n",
    "            _, hist = hypernet_test(model, env, 10, -1)\n",
    "\n",
    "            run_idx = model_file.split('.')[0]\n",
    "            fig, axs = plt.subplots(2, 5, figsize=(30, 16))\n",
    "            fig.suptitle(f\"Heterogeneous Environment Action Distribution: Run {run_idx}\", fontsize=24)\n",
    "\n",
    "            for type_idx in range(10):\n",
    "                row, col = divmod(type_idx, 5)\n",
    "                actions = hist[type_idx]\n",
    "                entropy = compute_action_entropy(actions, 10)\n",
    "                axs[row, col].hist(\n",
    "                    actions,\n",
    "                    bins=10,\n",
    "                    density=True,\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='black'\n",
    "                )\n",
    "                axs[row, col].set_title(f\"Type {type_idx}\", fontsize=20)\n",
    "                axs[row, col].set_xlabel(\"Action\", fontsize=16)\n",
    "                axs[row, col].set_ylabel(\"Frequency\", fontsize=16)\n",
    "                axs[row, col].set_xticks(range(env.n_actions))\n",
    "                axs[row, col].tick_params(axis='both', labelsize=14)\n",
    "                axs[row, col].grid(axis='y', alpha=0.5)\n",
    "                axs[row, col].text(0.5, -0.2, f'Entropy: {entropy:.2f}',\n",
    "                                   horizontalalignment='center', verticalalignment='top',\n",
    "                                   transform=axs[row, col].transAxes, fontsize=18)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.savefig(f\"{model_file}.jpg\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_file}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87c9e7",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a80eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# log_root = \"runs\\\\baseline_test_type_net\"\n",
    "# distribution_name = \"Eval/agent_total_rewards\"\n",
    "# OUTPUT_DIR = \"output_data\\\\type_net\"\n",
    "\n",
    "# log_root = \"runs\\\\batch_test_type_inf\"\n",
    "# distribution_name = \"Eval/agent_total_rewards\"\n",
    "# OUTPUT_DIR = \"output_data\\\\type_inf\"\n",
    "\n",
    "\n",
    "# log_root = \"runs\\\\batch_test_baseline\"\n",
    "# distribution_name = \"Eval/action_distribution\"\n",
    "# OUTPUT_DIR = \"output_data\\\\baseline_dists\"\n",
    "\n",
    "log_root = \"runs\\\\baseline_test_type_net\"\n",
    "distribution_name = \"Eval/action_distribution\"\n",
    "OUTPUT_DIR = \"output_data\\\\type_net_dists\"\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "subfolders = [os.path.join(log_root, f) for f in os.listdir(log_root) if os.path.isdir(os.path.join(log_root, f))]\n",
    "\n",
    "for folder in subfolders:\n",
    "    events_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.startswith(\"events.out.tfevents\")]\n",
    "    if not events_files:\n",
    "        continue\n",
    "\n",
    "    run_idx = os.path.basename(folder)\n",
    "\n",
    "    ea = event_accumulator.EventAccumulator(folder, size_guidance={event_accumulator.HISTOGRAMS: 1000})\n",
    "    ea.Reload()\n",
    "\n",
    "    if distribution_name not in ea.Tags().get('histograms', []):\n",
    "        continue\n",
    "\n",
    "    histograms = ea.Histograms(distribution_name)\n",
    "    latest = histograms[-1]  # most recent histogram\n",
    "    histogram = latest.histogram_value\n",
    "\n",
    "    bucket_limits = np.array(latest.histogram_value.bucket_limit)\n",
    "    bucket_counts = np.array(latest.histogram_value.bucket)\n",
    "\n",
    "    # Prepend a lower bound to get left/right edges of buckets\n",
    "    lower_bound = bucket_limits[0] - 1.0  # or use -np.inf\n",
    "    edges = np.concatenate([[lower_bound], bucket_limits])\n",
    "\n",
    "    # Compute midpoints for each bucket\n",
    "    mids = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "    # Now safely repeat each midpoint value according to the count in that bucket\n",
    "    data = np.repeat(mids, bucket_counts.astype(int))\n",
    "\n",
    "\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "\n",
    "    median = np.median(data)\n",
    "    min = np.min(data)\n",
    "    max = np.max(data)\n",
    "    bins = int(max - min) + 2\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=5, color='skyblue', edgecolor='black', density=False)\n",
    "    # plt.axvline(median, color='red', linestyle='dashed', linewidth=1.5, label=f\"Median: {median:.2f}\")\n",
    "    plt.title(f\"Action distribution for the Network Environment Run {run_idx}\")\n",
    "    # plt.legend()\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{run_idx}.jpg\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9d464",
   "metadata": {},
   "source": [
    "# Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6d785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for run 1 to output_data\\het_median rewards\\1.jpg\n",
      "Saved plot for run 10 to output_data\\het_median rewards\\10.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Configure to load scalars efficiently\u001b[39;00m\n\u001b[0;32m     39\u001b[0m ea \u001b[38;5;241m=\u001b[39m event_accumulator\u001b[38;5;241m.\u001b[39mEventAccumulator(\n\u001b[0;32m     40\u001b[0m     folder,\n\u001b[0;32m     41\u001b[0m     size_guidance\u001b[38;5;241m=\u001b[39m{event_accumulator\u001b[38;5;241m.\u001b[39mSCALARS: \u001b[38;5;241m10000\u001b[39m}\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m \u001b[43mea\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Create figure with subplots for each scalar\u001b[39;00m\n\u001b[0;32m     46\u001b[0m n_scalars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(scalar_names)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:344\u001b[0m, in \u001b[0;36mEventAccumulator.Reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator_mutex:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator\u001b[38;5;241m.\u001b[39mLoad():\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ProcessEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:567\u001b[0m, in \u001b[0;36mEventAccumulator._ProcessEvent\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summary_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;66;03m# This tensor summary was created using the old method that used\u001b[39;00m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;66;03m# plugin assets. We must still continue to support it.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m     tag \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mnode_name\n\u001b[1;32m--> 567\u001b[0m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwall_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatum\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:808\u001b[0m, in \u001b[0;36mEventAccumulator._ProcessHistogram\u001b[1;34m(self, tag, wall_time, step, histo)\u001b[0m\n\u001b[0;32m    806\u001b[0m histo_ev \u001b[38;5;241m=\u001b[39m HistogramEvent(wall_time, step, histo)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistograms\u001b[38;5;241m.\u001b[39mAddItem(tag, histo_ev)\n\u001b[1;32m--> 808\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompressed_histograms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAddItem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhisto_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CompressHistogram\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\reservoir.py:138\u001b[0m, in \u001b[0;36mReservoir.AddItem\u001b[1;34m(self, key, item, f)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutex:\n\u001b[0;32m    137\u001b[0m     bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buckets[key]\n\u001b[1;32m--> 138\u001b[0m \u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAddItem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\reservoir.py:221\u001b[0m, in \u001b[0;36m_ReservoirBucket.AddItem\u001b[1;34m(self, item, f)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutex:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 221\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mappend(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_items_seen)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:817\u001b[0m, in \u001b[0;36mEventAccumulator._CompressHistogram\u001b[1;34m(self, histo_ev)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_CompressHistogram\u001b[39m(\u001b[38;5;28mself\u001b[39m, histo_ev):\n\u001b[0;32m    813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Callback for _ProcessHistogram.\"\"\"\u001b[39;00m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompressedHistogramEvent(\n\u001b[0;32m    815\u001b[0m         histo_ev\u001b[38;5;241m.\u001b[39mwall_time,\n\u001b[0;32m    816\u001b[0m         histo_ev\u001b[38;5;241m.\u001b[39mstep,\n\u001b[1;32m--> 817\u001b[0m         \u001b[43mcompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_histogram_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhisto_ev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compression_bps\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    820\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\thesis2\\Lib\\site-packages\\tensorboard\\plugins\\distribution\\compressor.py:70\u001b[0m, in \u001b[0;36mcompress_histogram_proto\u001b[1;34m(histo, bps)\u001b[0m\n\u001b[0;32m     68\u001b[0m bucket \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(histo\u001b[38;5;241m.\u001b[39mbucket)\n\u001b[0;32m     69\u001b[0m bucket_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(histo\u001b[38;5;241m.\u001b[39mbucket_limit)\n\u001b[1;32m---> 70\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ===== USER CONFIGURATION =====\n",
    "# log_root = \"runs\\\\batch_test_baseline\"\n",
    "# scalar_names = [\"Eval/median_rewards\",]  # List of scalars to plot\n",
    "# OUTPUT_DIR = \"output_data\\\\baseline\"\n",
    "\n",
    "# OUTPUT_DIR = \"output_data\"\n",
    "# log_root = \"runs\\\\s\\\\hypernet-8000\"\n",
    "\n",
    "# OUTPUT_DIR = \"output_data\"\n",
    "# log_root = \"runs\\\\y\"\n",
    "\n",
    "# OUTPUT_DIR = \"output_data\\\\baseline_loss\"\n",
    "# scalar_names = [\"Actor/Metrics/Actor Loss\", \"Actor/Metrics/Critic Loss\"]\n",
    "# log_root = \"runs\\\\batch_test_baseline\"\n",
    "\n",
    "OUTPUT_DIR = \"output_data\\\\het_median rewards\"\n",
    "scalar_names =  [\"Eval/median_rewards\",]\n",
    "log_root = \"runs\\\\baseline_het\"\n",
    "# ==============================\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "subfolders = [os.path.join(log_root, f) for f in os.listdir(log_root) \n",
    "              if os.path.isdir(os.path.join(log_root, f))]\n",
    "\n",
    "for folder in subfolders:\n",
    "    events_files = [os.path.join(folder, f) for f in os.listdir(folder) \n",
    "                   if f.startswith(\"events.out.tfevents\")]\n",
    "    if not events_files:\n",
    "        continue\n",
    "\n",
    "    run_idx = os.path.basename(folder)\n",
    "    \n",
    "    # Configure to load scalars efficiently\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        folder,\n",
    "        size_guidance={event_accumulator.SCALARS: 10000}\n",
    "    )\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Create figure with subplots for each scalar\n",
    "    n_scalars = len(scalar_names)\n",
    "    fig, axes = plt.subplots(n_scalars, 1, figsize=(10, 4 * n_scalars), sharex=True)\n",
    "    \n",
    "    # If only one scalar, convert axes to list for consistent processing\n",
    "    if n_scalars == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Track if we found any valid scalars\n",
    "    found_any = False\n",
    "    \n",
    "    for i, scalar_name in enumerate(scalar_names):\n",
    "        # Check if scalar exists in this run\n",
    "        if scalar_name not in ea.Tags().get('scalars', []):\n",
    "            print(f\"Scalar '{scalar_name}' not found in run: {run_idx}\")\n",
    "            continue\n",
    "            \n",
    "        found_any = True\n",
    "        # Extract all scalar events\n",
    "        scalar_events = ea.Scalars(scalar_name)\n",
    "        steps = [event.step for event in scalar_events]\n",
    "        values = [event.value for event in scalar_events]\n",
    "        \n",
    "        # Plot to the current subplot\n",
    "        ax = axes[i]\n",
    "        ax.plot(steps, values, 'b-', linewidth=1.5)\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if not found_any:\n",
    "        plt.close(fig)\n",
    "        print(f\"No valid scalars found for run: {run_idx}\")\n",
    "        continue\n",
    "    \n",
    "    # Set common x-label at bottom\n",
    "    axes[-1].set_xlabel(\"Training Step\", labelpad=10)\n",
    "    \n",
    "    # Set main title for the entire figure with more space\n",
    "    fig.suptitle(f\"Median rewards observed in the Heterogeneous Environment. Run {run_idx}\", fontsize=16, y=0.98)\n",
    "    \n",
    "    # Adjust layout with more space at the top\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Increased space above subplots\n",
    "    \n",
    "    # Save plot\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{run_idx}.jpg\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved plot for run {run_idx} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34493c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged plot for scalar 'Actor/Metrics/Actor Loss' to output_data\\baseline_loss\\Actor_Metrics_Actor Loss.jpg\n",
      "Saved merged plot for scalar 'Actor/Metrics/Critic Loss' to output_data\\baseline_loss\\Actor_Metrics_Critic Loss.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ===== USER CONFIGURATION =====\n",
    "OUTPUT_DIR = \"output_data\\\\baseline_loss\"\n",
    "scalar_names = [\"Actor/Metrics/Actor Loss\", \"Actor/Metrics/Critic Loss\"]\n",
    "log_root = \"runs\\\\batch_test_baseline\"\n",
    "# ==============================\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "subfolders = [os.path.join(log_root, f) for f in os.listdir(log_root) \n",
    "              if os.path.isdir(os.path.join(log_root, f))]\n",
    "\n",
    "# Create one figure per scalar (each with a single plot containing all runs)\n",
    "for scalar_name in scalar_names:\n",
    "    # Create a new figure for this scalar\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Track if we have any data for this scalar\n",
    "    found_any = False\n",
    "    \n",
    "    for folder in subfolders:\n",
    "        run_idx = os.path.basename(folder)\n",
    "        events_files = [os.path.join(folder, f) for f in os.listdir(folder) \n",
    "                       if f.startswith(\"events.out.tfevents\")]\n",
    "        if not events_files:\n",
    "            continue\n",
    "            \n",
    "        # Load TensorBoard data\n",
    "        ea = event_accumulator.EventAccumulator(\n",
    "            folder,\n",
    "            size_guidance={event_accumulator.SCALARS: 10000}\n",
    "        )\n",
    "        ea.Reload()\n",
    "        \n",
    "        # Check if scalar exists\n",
    "        if scalar_name not in ea.Tags().get('scalars', []):\n",
    "            print(f\"Scalar '{scalar_name}' not found in run: {run_idx}\")\n",
    "            continue\n",
    "            \n",
    "        found_any = True\n",
    "        # Extract scalar data\n",
    "        scalar_events = ea.Scalars(scalar_name)\n",
    "        steps = [event.step for event in scalar_events]\n",
    "        values = [event.value for event in scalar_events]\n",
    "        \n",
    "        # Plot this run's data with run_idx as label\n",
    "        plt.plot(steps, values, label=run_idx, linewidth=1.0)\n",
    "    \n",
    "    if not found_any:\n",
    "        plt.close()\n",
    "        print(f\"No data found for scalar '{scalar_name}' in any run.\")\n",
    "        continue\n",
    "        \n",
    "    # Configure plot appearance\n",
    "    plt.title(\"Consolidated Plot of Actor (Top) and Critic (Bottom) Loss for models on the Simple Environment\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best', ncol=2, fontsize='x-small')  # Compact multi-column legend\n",
    "    \n",
    "    # Save scalar plot\n",
    "    safe_name = scalar_name.replace(\"/\", \"_\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{safe_name}.jpg\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved merged plot for scalar '{scalar_name}' to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17780",
   "metadata": {},
   "source": [
    "# ABM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b521404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = \"output_data\\\\abm\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "env = initialize_sir_env()\n",
    "parameters = ParameterSettings(\n",
    "    n_agents=env.n_agents,\n",
    "    d_action=env.n_actions,\n",
    "    d_obs=env.obs_size,\n",
    "    d_traits=env.d_traits,\n",
    "    d_het_latent=4,\n",
    "    d_beliefs=env.d_beliefs,\n",
    "    d_relation=env.d_relation,\n",
    "    d_message=4,\n",
    "    d_comm_state=env.d_comm_state,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "for i in range(0, 20): \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    checkpoint_steps = [35, 60, 120, 200]\n",
    "    colors = ['red', 'blue', 'green', 'purple']  # Different color for each checkpoint\n",
    "\n",
    "    seed = int(np.random.uniform(0, 100000))\n",
    "    for step, color in zip(checkpoint_steps, colors):\n",
    "        # Load model and run evaluation\n",
    "        model = PPOModel(parameters)\n",
    "        model = load_model(model, f\"foo\\\\si model\\\\checkpoint_ppo_step_{step}.pt\")\n",
    "        env = initialize_sir_env(seed=seed, eps_length=100)\n",
    "        _, all_episode_stats = si_eval_loop(model, env, 1, 1.0)\n",
    "        \n",
    "        # Extract infection data\n",
    "        episode_stats = all_episode_stats[0]\n",
    "        timesteps = range(len(episode_stats))\n",
    "        infected_counts = [s['infected'] for s in episode_stats]\n",
    "        \n",
    "        # Plot with checkpoint-specific label and color\n",
    "        plt.plot(timesteps, infected_counts, marker='o', linestyle='-', \n",
    "                label=f'Step {step}', color=color)\n",
    "\n",
    "    # Configure unified plot\n",
    "    plt.title(f'Infection Over Time on the SI Environment Run {i+1}')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Infected Individuals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Checkpoint Step')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot with iteration number + 1 as filename\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{i+1}.jpg\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    print(f\"Saved plot {i+1}.jpg to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
